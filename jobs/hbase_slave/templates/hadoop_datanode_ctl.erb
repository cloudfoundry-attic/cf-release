#!/bin/bash -e

LOG_DIR="/var/vcap/sys/log/hbase_slave"
RUN_DIR="/var/vcap/sys/run/hbase_slave"

CLEANUP_SCRIPT="/etc/cron.d/cleanup_logs_tmpdir"
HADOOP_BIN="/var/vcap/packages/hadoop/bin"
HADOOP_DATA_DIR="/var/vcap/store/hbase_master"
PIDFILE="${RUN_DIR}/hadoop-vcap-datanode.pid"
TMP_DIR="/var/vcap/store/hbase"

export HADOOP_CONF_DIR="/var/vcap/jobs/hbase_slave/config/hadoop"
export HADOOP_DATANODE_USER="vcap"
export HADOOP_IDENT_STRING="vcap"
export HADOOP_LOG_DIR=${LOG_DIR}
export HADOOP_PID_DIR=${RUN_DIR}
export JAVA_HOME="/var/vcap/packages/dea_jvm"

source /var/vcap/packages/common/utils.sh

case $1 in

  start)
    # hadoop-daemon.sh has its own pid guard,
    # we use ours for consistency anyway
    pid_guard $PIDFILE "hadoop_datanode"

    mkdir -p $RUN_DIR
    mkdir -p $LOG_DIR
    mkdir -p $HADOOP_DATA_DIR
    mkdir -p $TMP_DIR

    chown vcap:vcap $RUN_DIR
    chown vcap:vcap $LOG_DIR
    chown vcap:vcap $HADOOP_DATA_DIR
    chown vcap:vcap $TMP_DIR

    # Set maximum number of open file descriptors.
    ulimit -n 32768
    # Set maximum number of processes available to a single user.
    ulimit -u 32000

    # Blow away logs directory once every 10 minutes if it's over 1500M
    echo "*/10 * * * * root $BIN_DIR/cleanup_tmpdir $LOG_DIR 1500" > $CLEANUP_SCRIPT

    # hadoop ctl scripts manage their pidfiles,
    # so we don't attempt to write ours
    exec chpst -u vcap:vcap $HADOOP_BIN/hadoop-daemon.sh --config $HADOOP_CONF_DIR start datanode \
         >>$LOG_DIR/hadoop_datanode_start.stdout.log \
         2>>$LOG_DIR/hadoop_datanode_start.stderr.log

    ;;

  stop)
    exec chpst -u vcap:vcap $HADOOP_BIN/hadoop-daemon.sh --config $HADOOP_CONF_DIR stop datanode \
         >>$LOG_DIR/hadoop_datanode_stop.stdout.log \
         2>>$LOG_DIR/hadoop_datanode_stop.stderr.log

    ;;

  *)
    echo "Usage: hadoop_datanode_ctl {start|stop}"

    ;;

esac
